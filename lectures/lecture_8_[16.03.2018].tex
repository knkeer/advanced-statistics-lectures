%!TEX root = ../main.tex

\section{Лекция 8}
\subsection{Критерий факторизации Неймана-Фишера: продолжение}
На предыдущей лекции был сформулирован критерий достаточности статистики $S(\vec{X})$, называемый критерием факторизации Неймана-Фишера. Но он был доказан только в дискретном случае, так как для доказательства в непрерывном случае требовалась формула пересчёта условных матожиданий. Теперь она есть и доказана, поэтому можно приступить и к критерию.

Доказательство, данное ниже, будет корректно для всех случаев, но мы сконцентрируемся на непрерывном случае.
\begin{proof}
	Зафиксируем некоторое $\theta' \in \Theta$ и для любого $\theta \neq \theta'$ введём следующую вероятностную меру:
	\[
		\Pr_{\theta, \theta'} = \frac{\Pr_{\theta} + \Pr_{\theta'}}{2}.
	\]
	Сразу же заметим, что $\Pr_{\theta} \ll \Pr_{\theta, \theta'}$ и $\Pr_{\theta'} \ll \Pr_{\theta, \theta'}$. Тогда по теореме Радона-Никодима существуют производные
	\begin{align*}
		\frac{\dd \Pr_{\theta}}{\dd \Pr_{\theta, \theta'}}(x)
		&= \frac{2p_{\theta}(x)}{p_{\theta}(x) + p_{\theta'}(x)} 
		\equiv f_{\theta}(x), \\
		\frac{\dd \Pr_{\theta'}}{\dd \Pr_{\theta, \theta'}}(x)
		&= \frac{2p_{\theta'}(x)}{p_{\theta}(x) + p_{\theta'}(x)} 
		\equiv f'_{\theta}(x).
	\end{align*}
	Теперь выразим плотность $p_{\theta}(x)$ через $f_{\theta}(x)$:
	\[
		p_{\theta}(x) 
		= p_{\theta'}(x)\frac{f_{\theta}(x)}{2 - f_{\theta}(x)}.
	\]
	Если доказать, что $f_{\theta}(x)$ есть функция от $S(x)$ то мы сразу же получим, что $p_{\theta}(x) = h(x)\psi_{\theta}(S(x))$, где $h(x) = p_{\theta'}(x)$, а $\psi_{\theta}(S(x)) = f_{\theta}(x)/(2 - f_{\theta}(x))$. Это план доказательства в прямую сторону.

	Какой же может быть план доказательства в обратную сторону? Пусть имеет место факторизация: $p_{\theta}(x) = h(x)\psi_{\theta}(S(x))$. Тогда
	\[
		f_{\theta}(x) 
		= \frac{2h(x)\psi_{\theta}(S(x))}{h(x)\psi_{\theta}(S(x)) + h(x)\psi_{\theta'}(S(x))}
		= \frac{2\psi_{\theta}(S(x))}{\psi_{\theta}(S(x)) + \psi_{\theta'}(S(x))}.
	\]
	Следовательно, $f_{\theta}(x)$ есть функция от $S(x)$ и $\theta$. После чего этим можно будет воспользоваться.

	Для начала докажем в обратную сторону. Пусть имеет место факторизация. Тогда $f_{\theta}(x)$ и $f'_{\theta}(x)$ есть функции от $S(x)$. Воспользуемся формулой пересчёта условного математического ожидания $T(\vec{X})$ по $S(\vec{X})$:
	\begin{align*}
		\EE_{\theta}[T(\vec{X})\,|\,S(\vec{X})]
		&= \frac{\EE_{\theta, \theta'}[T(\vec{X})f_{\theta}(\vec{X})\,|\,S(\vec{X})]}{\EE_{\theta, \theta'}[f_{\theta}(\vec{X})\,|\,S(\vec{X})]}
		= \frac{f_{\theta}(\vec{X})\EE_{\theta, \theta'}[T(\vec{X})\,|\,S(\vec{X})]}{f_{\theta}(\vec{X})} \\
		&= \EE_{\theta, \theta'}[T(\vec{X})\,|\,S(\vec{X})].
	\end{align*}
	Аналогично получаем, что $\EE_{\theta'}[T(\vec{X})\,|\,S(\vec{X})] = \EE_{\theta, \theta'}[T(\vec{X})\,|\,S(\vec{X})]$. Следовательно, для любого $\theta \neq \theta'$ $\EE_{\theta}[T(\vec{X})\,|\,S(\vec{X})] = \EE_{\theta'}[T(\vec{X})\,|\,S(\vec{X})]$. Теперь возьмём $T(\vec{X}) = \mathbf{1}_{\vec{X} \in B}$, где $B \in \mathcal{B}(\mathbb{R}^{n})$. Отсюда получаем, что условное распределение $\vec{X}$ относительно $S(\vec{X})$ не зависит от $\theta$, но это и одначает достаточность статистики $S(\vec{X})$.

	Приступим к доказательству в прямую сторону. Нам нужно доказать, что $f_{\theta}(x)$ есть $S(x)$-измеримая функция. Для этого покажем, что $\EE_{\theta}[f_{\theta}(x)\,|\,S(\vec{X})] = f_{\theta}(\vec{X})$. В силу достаточности статистики $S(\vec{X})$ $\EE_{\theta}[f_{\theta}(x)\,|\,S(\vec{X})]$ не зависит от $\theta$ и
	\[
		\EE_{\theta'}[f_{\theta}(x)\,|\,S(\vec{X})]
		= \EE_{\theta}[f_{\theta}(x)\,|\,S(\vec{X})].
	\]
	Покажем, что $\EE_{\theta, \theta'}[f_{\theta}(x)\,|\,S(\vec{X})] = \EE_{\theta}[f_{\theta}(x)\,|\,S(\vec{X})]$. Для этого проверим интегральное свойство. Зафиксируем произвольное борелевское множество $B \in \mathcal{B}(\mathbb{R}^{k})$ (будем считать, что $S(\vec{X}) \in \mathbb{R}^{k}$) и распишем: 
	\begin{align*}
		\EE_{\theta, \theta'}[f_{\theta}(x)\mathbf{1}_{S(\vec{X}) \in B}]
		&= \frac{1}{2}\EE_{\theta}[f_{\theta}(x)\mathbf{1}_{S(\vec{X}) \in B}] + \frac{1}{2}\EE_{\theta'}[f_{\theta}(x)\mathbf{1}_{S(\vec{X}) \in B}].
	\end{align*}
	Далее, по интегральному свойству
	\begin{align*}
		\EE_{\theta, \theta'}[f_{\theta}(x)\mathbf{1}_{S(\vec{X}) \in B}]
		&= \frac{1}{2}\EE_{\theta}[\EE_{\theta}[f_{\theta}(x)\,|\,S(\vec{X})]\mathbf{1}_{S(\vec{X}) \in B}] + \frac{1}{2}\EE_{\theta'}[\EE_{\theta'}[f_{\theta}(x)\,|\,S(\vec{X})]\mathbf{1}_{S(\vec{X}) \in B}] \\
		&= \frac{1}{2}\EE_{\theta}[\EE_{\theta}[f_{\theta}(x)\,|\,S(\vec{X})]\mathbf{1}_{S(\vec{X}) \in B}] + \frac{1}{2}\EE_{\theta'}[\EE_{\theta}[f_{\theta}(x)\,|\,S(\vec{X})]\mathbf{1}_{S(\vec{X}) \in B}] \\
		&= \EE_{\theta, \theta'}[\EE_{\theta}[f_{\theta}(x)\,|\,S(\vec{X})]\mathbf{1}_{S(\vec{X}) \in B}].
	\end{align*}
	Тем самым получаем желаемое. Но вопрос: а зачем нам это нужно? Затем, чтобы применить формулу пересчёта условных математических ожиданий. Применим её к статистике $f_{\theta}(\vec{X})$:
	\[
		\EE_{\theta}[f_{\theta}(x)\,|\,S(\vec{X})]
		= \frac{\EE_{\theta, \theta'}[f_{\theta}^{2}(x)\,|\,S(\vec{X})]}{\EE_{\theta, \theta'}[f_{\theta}(x)\,|\,S(\vec{X})]}
	\]
	Но тогда мы получаем интересную вещь:
	\[
		\EE_{\theta, \theta'}[f_{\theta}^{2}(x)\,|\,S(\vec{X})]
		= (\EE_{\theta, \theta'}[f_{\theta}(x)\,|\,S(\vec{X})])^{2}.
	\]
	Однако нам известно, что для любых случайных величин $\xi$ и $\eta$ $\DD[\xi\,|\,\eta] = \EE[(\xi - \EE[\xi\,|\,\eta])^{2}\,|\,\eta] = \EE[\xi^{2}\,|\,\eta] - (\EE[\xi\,|\,\eta])^{2}$. Тогда
	\[
		\EE_{\theta, \theta'}[(f_{\theta}(x) - \EE_{\theta, \theta'}[f_{\theta}(x)\,|\,S(\vec{X})])^{2}\,|\,S(\vec{X})] = 0.
	\]
	Взяв матожидание $\EE_{\theta, \theta'}$, получим, что $\EE_{\theta, \theta'}[f_{\theta}(x)\,|\,S(\vec{X})] = f_{\theta}(x)$. Следовательно, $\EE_{\theta}[f_{\theta}(x)\,|\,S(\vec{X})] = f_{\theta}(\vec{X})$ и получается искомое разложение $p_{\theta}(x)$, что и требовалось доказать.
\end{proof}

Попробуем применить этот критерий. Ранее мы находили оптимальную оценку для схемы Бернулли. Теперь найдём её для равномерного распределения.
\begin{problem}
	Пусть $\vec{X} = (X_{1}, \ldots, X_{n})$ "--- выборка из равномерного распределения $\Uniform[0, \theta]$, $\theta > 0$. Найти оптимальную оценку $\theta$.
\end{problem}
\begin{proof}
	План действий следующий: нужно найти достаточную статистику, проверить её на полноту, после чего решить уравнение несмещённости. Первое сделать сосем несложно, пользуясь критерием факторизации. Распишем плотность выборки:
	\[
		p_{\theta}(\vec{X})
		= \prod_{k = 1}^{n} p_{\theta}(X_{k})
		= \prod_{k = 1}^{n} \frac{1}{\theta}\mathbf{1}_{0 \leq X_{k} \leq \theta}
		= \frac{1}{\theta^{n}}\mathbf{1}_{0 \leq X_{(1)} \leq X_{(n)} \leq \theta}.
	\]
	Достаточной статистикой в данном случае будет являться $X_{(n)}$, так как в критерии факторизации можно взять $h(\vec{X}) = \mathbf{1}_{0 \leq X_{(1)} \leq X_{(n)}}$, $\psi_{\theta}(S(\vec{X}) = \theta^{-n}\mathbf{1}_{X_{(n)} \leq \theta}$. 

	Небольшое лирическое отступление: а почему мы рассматриваем именно эту статистику? Достаточных статистик полно "--- как минимум, сама выборка будет достаточной. Но есть такая <<рекомендация>>: если параметр $k$–мерный, то статистику тоже подбирают $k$-мерной. Но так можно сделать не всегда. Например, если взять $\mathcal{N}(\mu, \sigma^{2})$, то достаточной статистикой будет $(\sum_{k = 1}^{n} X_{k}, \sum_{k = 1}^{n} X_{k}^{2})$. Однако, если взять распределение $\mathcal{N}(\theta^{2}, \theta^{2})$, то связь между $\mu$ и $\sigma^{2}$ есть, а связь между $\sum_{k = 1}^{n} X_{k}$ и $\sum_{k = 1}^{n} X_{k}^{2}$ построить особо не получится. Тем самым получается, что для одномерного параметра $\theta$ статистика $(\sum_{k = 1}^{n} X_{k}, \sum_{k = 1}^{n} X_{k}^{2})$ будет двумерной достаточной, но она не будет полной.

	Вернёмся к задаче и покажем, что $X_{(n)}$ есть полная статистика. Для этого вспомним, что плотность максимума равна
	\[
		p_{X_{(n)}}(x) 
		= \frac{nx^{n - 1}}{\theta^{n}}\mathbf{1}_{x \in [0, \theta]}.
	\]
	Посчитаем матожидание произвольной функции $g(X_{(1)})$:
	\[
		\EE_{\theta}[g(X_{(n)})]
		= \int_{0}^{\theta} g(x)\frac{nx^{n - 1}}{\theta^{n}}\dd x.
	\]
	Теперь предположим, что для всех $\theta > 0$ $\EE_{\theta}[g(X_{(n)})] = 0$. Это равносильно тому, что
	\[
		\int_{0}^{\theta} g(x)x^{n - 1}\dd x = 0.
	\]
	Продифференцируем по $\theta$:
	\[
		g(\theta)\theta^{n - 1} 
		= 0
		\implies
		g(\theta) 
		= 0 \text{ для всех } \theta > 0.
	\]
	Тогда $g(X_{(1)}) = 0$ $\Pr_{\theta}$-п.н. для всех $\theta > 0$ и $X_{(n)}$ есть полная достаточная статистика. Осталось решить уравнение несмещённости: $\EE_{\theta}[\phi(X_{(1)})] = \theta$. Заметим, что это равносильно тому, что:
	\[
		\frac{\theta^{n + 1}}{n}
		= \int_{0}^{\theta} \phi(x)x^{n - 1}\dd x.
	\]
	Продифференцируем по $\theta$:
	\[
		\frac{n + 1}{n}\theta^{n} 
		= \phi(\theta)\theta^{n - 1}
		\implies
		\phi(\theta) = \frac{n + 1}{n}\theta.
	\]
	Следовательно, оптимальной оценкой $\theta$ является $(1 + n^{-1})X_{(n)}$.
\end{proof}

Что ещё бывает здесь полезного? Например, можно считать условные математические ожидания через полные достаточные статистики. Рассмотрим какой-нибудь плохой пример.
\begin{problem}
	Пусть $\vec{X} = (X_{1}, \ldots, X_{n})$ "--- выборка из стандартного равномерного распределения $\Uniform[0, 1]$. Вычислить $\EE[\sqrt{X_{1} + X_{2}}\,|\,X_{(n)}]$.
\end{problem}
\begin{proof}
	Как бы мы могли считать такую вещь? Можно было бы попытаться посчитать условную плотность, но никто не гарантирует её существования, да и функции очень плохие "--- вычисления будут отвратительными. Попробуем свести её к статистической задаче. Для этого скажем, что мы смотрим не на стандартное равномерное распределение, а на $\Uniform[0, \theta]$, где $\theta > 0$ (потом просто подставим $\theta = 1$). В таком случае известно, что $X_{(n)}$ есть полная достаточная статистика в равномерной модели $\Uniform[0, \theta]$. Тогда $\EE_{\theta}[\sqrt{X_{1} + X_{2}}\,|\,X_{(n)}]$ не зависит от $\theta$, а зависит только от $X_{(n)}$. И чем же это является? Ответ такой: оптимальной оценкой своего математического ожидания, то есть
	\[
		\EE_{\theta}[\EE_{\theta}[\sqrt{X_{1} + X_{2}}\,|\,X_{(n)}]]
		= \EE_{\theta}[\sqrt{X_{1} + X_{2}}].
	\]
	Осталось посчитать это матожидание:
	\begin{align*}
		\EE_{\theta}[\sqrt{X_{1} + X_{2}}]
		&= \int_{0}^{\theta}\int_{0}^{\theta} \sqrt{x + y}\frac{1}{\theta^{2}}\dd x \dd y
		= \frac{1}{\theta^{2}}\int_{0}^{\theta}\left(\int_{0}^{\theta} \sqrt{x + y}\dd x\right)\dd y \\
		&= \frac{1}{\theta^{2}}\int_{0}^{\theta} \frac{2}{3}(y + \theta)^{3/2}\dd y
		= \frac{1}{\theta^{2}}\frac{2}{3}\frac{2}{5}(2\theta)^{5/2}
		= \frac{16\sqrt{2}}{15}\sqrt{\theta}.
	\end{align*}
	Тогда получаем, что
	\[
		\EE_{\theta}[\sqrt{X_{1} + X_{2}}\,|\,X_{(n)}]
		= \frac{16\sqrt{2}}{15}\sqrt{X_{(n)}}. \qedhere
	\]
\end{proof}

Этот метод позволяет быстро считать условные матожидания, но он работает только тогда, когда в условии стоит полная достаточная статистика. На этом с достаточностью заканчиваем.

\subsection{Полнота}
Если мы ищем достаточную статистику, то мы параллельно проверяем её на полноту. Но как это делать? В случае схемы Бернулли или же равномерного проверки были сделана честно, но в общем случае это достаточно трудно.

Это достаточно тяжёлый вопрос. Для достаточности есть достаточно общий критерий факторизации, который даёт ответ в большинстве разумных случаев. Для полноты же такого удобного критерия нет. Однако в некоторых хороших случаях можно вывести условия для полноты "--- например, если семейство распределений является экспоненциальным.
\begin{definition}
	Пусть $\set{\Pr_{\vec{\theta}}\,|\,\vec{\theta} \in \Theta}$ "--- семейство распределений с $k$-мерным параметром: $\Theta \subseteq \mathbb{R}^{k}$, $k \in \mathbb{N}$. Далее, пусть оно доминируемо с плотностью $p_{\vec{\theta}}(x)$ по мере $\mu$. Если эта плотность представима в следующем виде:
	\[
		p_{\vec{\theta}}(x) = h(x)\exp\left\{\sum_{i = 1}^{k}u_{i}(x)a_{j}(\vec{\theta}) + v(\vec{\theta})\right\},
	\]
	где $h$ и $u_{i}$ "--- это борелевские функции, то семейство распределений $\set{\Pr_{\vec{\theta}}\,|\,\vec{\theta} \in \Theta}$ называется \emph{экспоненциальным}.
\end{definition}
Ранее мы уже видели примеры семейств распределений, не являющихся экспоненциальным "--- то же равномерное, так как индикатор в виде экспоненты не представить.

Теперь предоложим, что семейство является экспоненциальным. Чему будет равна достаточная статистика? Понятно, что она будет равна $\vec{S}(x) = (u_{1}(x), \ldots, u_{k}(x))$. Оказывается, что если функции $a_{i}(\vec{\theta})$ достаточно хороши, то $\vec{u}(x)$ будет и полной.
\begin{theorem}[об экспоненциальном семействе]
	Пусть $\set{\Pr_{\vec{\theta}}\,|\,\vec{\theta} \in \Theta}$ "--- экспоненциальное семейство распределений, а функция $\vec{a}(\vec{\theta}) = (a_{1}(\vec{\theta}), \ldots, a_{k}(\vec{\theta}))$ такова, что при пробегании $\vec{\theta}$ всего $\Theta$ $\vec{a}(\vec{\theta})$ зачерчивает $k$-мерный параллелепипед (или шар). Тогда $\vec{S}(\vec{X}) = (u_{1}(\vec{X}), \ldots, u_{k}(\vec{X}))$ будет полной достаточной статистикой для $\vec{\theta}$.
\end{theorem}
Она обычно формулируется в таком виде, но есть вопрос: а важно ли то, что размерности параметра и статистики совпадают? В принципе, необязательно, но это достаточно разумное требование, так как иначе могут возникать не самые приятные подводные камни.

Доказательство данной теоремы требует две леммы, в доказательстве которых используется комплексный анализ. Первая из них описывает плотность $S(\vec{X})$, а вторая обычно называется теоремой единственности аналитической функции комплексного переменного. Начнём с плотности. Было бы неплохо сказать что-то про плотность $S(\vec{X})$, так как при рассуждении про полноту требуется распределение этой статистики.

Пусть 
\[
	\psi_{\theta}(S(\vec{X})) = \exp\left\{\sum_{i = 1}^{k}u_{i}(x)a_{j}(\vec{\theta}) + v(\vec{\theta})\right\},
\]
где $\vec{S}(\vec{X}) = (u_{1}(\vec{X}), \ldots, u_{k}(\vec{X}))$, то есть $p_{\theta}(\vec{X}) = h(\vec{X})\psi_{\theta}(\vec{S}(\vec{X}))$. 
\begin{lemma}
	Статистика $\vec{S}(\vec{X})$ имеет плотность $\psi_{\theta}(\vec{s})$ по мере
	\[
		\nu(B) = \int_{\vec{S}^{-1}(B)} h(\vec{x})\dd x \text{ (если } p_{\theta}(x) \text{ абсолютно непрерывна)}.
	\]
\end{lemma}
\begin{proof}
	Для существования плотности нужно показать, что распределение статистики $\vec{S}(\vec{X})$ абсолютно непрерывно относительно меры $\nu$. Пусть $G_{\theta}$ есть распределение $\vec{S}(\vec{X})$. Если $\nu(B) = 0$, то $h(\vec{x}) = 0$ для всех $\vec{x} \in \vec{S}^{-1}(B)$ (так как $h(x) \geq 0$). Тогда посмотрим на $\Pr_{\theta}(\vec{S}(\vec{X}) \in B)$. Заметим, что
	\begin{align*}
		\Pr_{\theta}(\vec{S}(\vec{X}) \in B)
		&= \EE_{\theta}[\mathbf{1}_{\vec{S}(\vec{X}) \in B}]
		= \int_{\mathbb{R}^{k}} \mathbf{1}_{\vec{S}(\vec{x}) \in B}p_{\theta}(\vec{x})\dd \vec{x}
		= \int_{\vec{S}^{-1}(B)} h(\vec{x})\psi_{\theta}(\vec{S}(\vec{x}))\dd \vec{x}
		= 0.
	\end{align*}
	Следовательно, распределение $G_{\theta}$ абсолютно непрерывно относительно $\nu$ и по теореме Радона-Никодима существует производная $\frac{\dd G_{\theta}}{\dd \nu}$. Осталось проверить, что она равна $\psi_{\theta}(\vec{s})$. Снова распишем $\Pr_{\theta}(\vec{S}(\vec{X}) \in B)$:
	\[
		\Pr_{\theta}(\vec{S}(\vec{X}) \in B)
		= \int_{\vec{S}^{-1}(B)} h(\vec{x})\psi_{\theta}(\vec{S}(\vec{x}))\dd \vec{x}.
	\]
	Теперь пересчитываем интеграл, делая замену $\vec{s} = \vec{S}(\vec{x})$:
	\[
		\Pr_{\theta}(\vec{S}(\vec{X}) \in B)
		= \int_{B} \psi_{\theta}(\vec{s})\nu(\dd \vec{s}). \qedhere
	\]
\end{proof}

Вторая лемма утверждает следующее:
\begin{lemma}[единственности аналитических функций]
	Пусть $G_{1}$ и $G_{2}$ "--- две меры Лебега такие, что существует параллелепипед $D \subseteq \mathbb{R}^{k}$ со следующим условием: для любого $\vec{a} \in D$
	\[
		\int_{\mathbb{R}^{k}} e^{\langle \vec{a}, \vec{y} \rangle}G_{1}(\dd \vec{y})
		= \int_{\mathbb{R}^{k}} e^{\langle \vec{a}, \vec{y} \rangle}G_{2}(\dd \vec{y})
		< \infty
	\]
	Тогда $G_{1} = G_{2}$.
\end{lemma}
\begin{proof}[Идея доказательства]
	<здесь была идея, но её я допишу позже>
\end{proof}

Теперь докажем теорему об экспоненциальном семействе.
\begin{proof}
	Достаточность статистики $\vec{S}(\vec{X})$ следует из критерия факторизации Неймана-Фишера. Покажем, что она будет полна. Пусть $\EE_{\vec{\theta}}[f(\vec{S}(\vec{X}))] = 0$ для всех $\vec{\theta} \in \Theta$. Заметим, что по лемме о плотности статистики
	\[
		\EE_{\vec{\theta}}[f(\vec{S}(\vec{X}))]
		= \int_{\mathbb{R}^{k}} f(\vec{s})\psi_{\vec{\theta}}(\vec{s})\nu(\dd \vec{s})
		= 0.
	\]
	Теперь введём две функции: $f^{+}(\vec{s}) = \max\{f(\vec{s}), 0\}$ и $f^{-}(\vec{s}) = \min\{-f(\vec{s}), 0\}$. Тогда для любого $\vec{\theta} \in \Theta$
	\[
		\int_{\mathbb{R}^{k}} f^{+}(\vec{s})\psi_{\vec{\theta}}(\vec{s})\nu(\dd \vec{s})
		= \int_{\mathbb{R}^{k}} f^{-}(\vec{s})\psi_{\vec{\theta}}(\vec{s})\nu(\dd \vec{s}).
	\]
	Теперь вспомним, чему равно $\psi_{\theta}(\vec{s})$. Тогда
	\[
		\int_{\mathbb{R}^{k}} f^{+}(\vec{s})\exp\left\{\langle \vec{a}(\vec{\theta}), \vec{s}\rangle + v(\vec{\theta})\right\}\nu(\dd \vec{s})
		= \int_{\mathbb{R}^{k}} f^{-}(\vec{s})\exp\left\{\langle \vec{a}(\vec{\theta}), \vec{s}\rangle + v(\vec{\theta})\right\}\nu(\dd \vec{s})
	\]
	Следовательно, существует некоторый параллелепипед $D \subseteq \mathbb{R}^{k}$ такой, что для любого $\vec{a} \in D$:
	\[
		\int_{\mathbb{R}^{k}} e^{\langle \vec{a}, \vec{s}\rangle} f^{+}(\vec{s}) \nu(\dd \vec{s})
		= \int_{\mathbb{R}^{k}} e^{\langle \vec{a}, \vec{s}\rangle} f^{-}(\vec{s}) \nu(\dd \vec{s}).
	\]
	Но это есть ни что иное, как лемма о единственности аналитической функции. Тогда 
	\[
		f^{+}(\vec{s}) \nu(\dd \vec{s})
		= f^{-}(\vec{s}) \nu(\dd \vec{s})
		\implies
		f^{+}(\vec{s}) 
		= f^{-}(\vec{s}) \text{ по мере } \nu.
	\]
	Так как $\vec{S}(\vec{X})$ имеет плотность по мере $\nu$, то $f(\vec{S}(\vec{X})) = 0$ $\Pr_{\vec{\theta}}$-п.н. для любого $\vec{\theta} \in \Theta$. Следовательно, $\vec{S}(\vec{X})$ полна.
\end{proof}

Теперь рассмотрим пример применения теоремы.
\begin{problem}
	Пусть $\vec{X} = (X_{1}, \ldots, X_{n})$ "--- выборка из нормального распределения $\mathcal{N}(\mu, \sigma^{2})$. Рассмотрим три случая:
	\begin{enumerate}[label=(\alph*)]
		\item Среднее $\mu$ неизвестно, дисперсия $\sigma^{2}$ известна: $\mu = \theta$, $\theta \in \mathbb{R}$;
		\item Среднее $\mu$ известно, дисперсия $\sigma^{2}$ неизвестна: $\sigma^{2} = \theta$, $\theta \in \mathbb{R}_{++}$;
		\item Ни среднее $\mu$, ни дисперсия $\sigma^{2}$ неизвестны: $\vec{\theta} = (\mu, \sigma^{2})$, $\vec{\theta} \in \mathbb{R} \times \mathbb{R}_{++}$.
	\end{enumerate}
	Найти оптимальные оценки параметра $\theta$.
\end{problem}
\begin{proof}
	Для начала выпишем плотность выборки:
	\[
		p_{\theta}(\vec{X})
		= \prod_{k = 1}^{n} p_{\theta}(X_{k})
		= \prod_{k = 1}^{n} \frac{1}{\sigma\sqrt{2\pi}}\exp\left\{-\frac{(X_{k} - \mu)^{2}}{2\sigma^{2}}\right\}
		= \frac{1}{(2\pi)^{n/2}\sigma^{n}}\exp\left\{-\frac{1}{2\sigma^{2}}\sum_{k = 1}^{n} (X_{k} - \mu)^{2}\right\}.
	\]

	Теперь будем решать пункты по отдельности.
	\begin{enumerate}[label=(\alph*)]
		\item Заметим, что
		\[
			\sum_{k = 1}^{n} (X_{k} - \mu)^{2} 
			= \sum_{k = 1}^{n} X_{k}^{2} - 2\mu\sum_{k = 1}^{n} X_{k} + n\mu^{2}.
		\]
		Тогда
		\[
			p_{\theta}(\vec{X})
			= \frac{1}{(2\pi)^{n/2}}\exp\left\{-\frac{1}{2\sigma^{2}}\sum_{k = 1}^{n} X_{k}^{2} + \frac{\mu}{\sigma^{2}}\sum_{k = 1}^{n} X_{k} - \frac{n\mu^{2}}{2\sigma^{2}} - \frac{n}{2}\ln \sigma^{2}\right\}.
		\]
		Следовательно, это экспоненциальное семейство, в котором $a_{1}(\mu) = \mu/(2\sigma^{2})$, $u_{1}(\vec{X}) = \sum_{k = 1}^{n} X_{k}$, $h(\vec{X}) = (2\pi)^{-n/2}\exp\left\{-(2\sigma)^{-1}\sum_{k = 1}^{n} X_{k}^{2}\right\}$, а $v(\mu) = -n\mu^{2}/(2\sigma^{2}) - n\ln\sigma$. Так как $\theta$ пробегает всю прямую, то $a_{1}(\mu)$ тоже пробегает всю прямую, а, следовательно, и какой-то отрезок. Тогда $u_{1}(\vec{X})$ есть полная достаточная статистика. Теперь достаточно решить уравнение несмещённости, но его уже решали очень много раз "--- подходит выборочное среднее $\overline{\vec{X}}$, что и будет оптимальной оценкой среднего.

		\item В данном случае никакие преобразования не требуются. Это будет экспоненциальное семейство, в котором $a_{1}(\theta) = -1/(2\sigma^{2})$, $u_{1}(\vec{X}) = \sum_{k = 1}^{n} (X_{k} - \mu)^{2}$, $v(\theta) = -n\ln\sigma$, $h(\vec{X}) = (2\pi)^{-n/2}$. В данном случае $a_{1}(\theta)$ пробегает $(-\infty, 0)$, поэтому $u_{1}(\vec{X})$ есть полная достаточная статистика. Теперь нужно решить уравнение несмещённости. Для этого заметим, что
		\[
			\EE_{\theta}\left[\sum_{k = 1}^{n} (X_{k} - \mu)^{2}\right]
			= n\DD_{\theta}[X_{1}]
			= n\sigma^{2}.
		\]
		Тогда оптимальной оценкой $\sigma^{2}$ будет $n^{-1}\sum_{k = 1}^{n} (X_{k} - \mu)^{2}$.

		\item В данном случае запишем плотность следующим образом:
		\[
			p_{\vec{\theta}}(\vec{X})
			= \frac{1}{(2\pi)^{n/2}}\exp\left\{-\frac{1}{2\sigma^{2}}\sum_{k = 1}^{n} X_{k}^{2} + \frac{\mu}{\sigma^{2}}\sum_{k = 1}^{n} X_{k} - \frac{n\mu^{2}}{2\sigma^{2}} - \frac{n}{2}\ln \sigma^{2}\right\}.
		\]
		Тогда это будет экспоненциальное семейство распределений с $\vec{a}(\vec{\theta}) = (\mu/(2\sigma^{2}), -1/(2\sigma^{2}))$, $\vec{u}(\vec{X}) = (\sum_{k = 1}^{n} X_{k}, \sum_{k = 1}^{n} X_{k}^{2})$, $v(\vec{\theta}) = -n\mu^{2}/(2\sigma^{2}) - n\ln\sigma$, $h(\vec{X}) = (2\pi)^{-n/2}$. Заметим, что $\vec{a}(\vec{\theta})$ пробегает $\mathbb{R} \times \mathbb{R}_{++}$. Тогда $\vec{u}(\vec{X})$ есть полная и достаточная статистика. Теперь надо решить уравнения несмещённости. Сразу же скажем, что $\EE_{\vec{\theta}}[\overline{\vec{X}}] = \mu$, то есть выборочное среднее есть оптимальная оценка среднего. Далее,
		\[
			\EE_{\vec{\theta}}\left[\frac{1}{n}\sum_{k = 1}^{n} X_{k}^{2}\right] = \mu^{2} + \sigma^{2}
		\]
		Нужно избавиться от $\mu^{2}$. Для этого посчитаем матожидание квадрата выборочно среднего, пользуясь тем, что $\overline{\vec{X}} \sim \mathcal{N}(\mu, \sigma^{2}/n)$:
		\[
			\EE_{\vec{\theta}}[(\overline{\vec{X}})^{2}]
			= \mu^{2} + \frac{\sigma^{2}}{n}.
		\]
		Поэтому оптимальной оценкой $\sigma^{2}$ будет
		\[
			S^{2}
			= \frac{1}{n - 1}\sum_{k = 1}^{n}(X_{k} - \overline{\vec{X}})^{2}. \qedhere
		\]
	\end{enumerate}
\end{proof}