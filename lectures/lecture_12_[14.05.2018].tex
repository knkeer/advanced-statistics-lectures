%!TEX root = ../main.tex

\section{Лекция 12}
\subsection{Критерий согласия}
Рассмотрим так называемые критерии согласия. Такие критерии предлагают согласиться или же не согласиться с чем-то, что имеет простую природу. 
\begin{definition}
	Пусть $\vec{X} = (X_{1}, \ldots, X_{n})$ "--- функция распределения с неизвестной функцией распределения $F(x)$. Далее, введём гипотезы $\mathrm{H}_{0} \colon F = F_{0}$ и $\mathrm{H}_{1} \colon F \neq F_{0}$. Критерий для проверки гипотезы $\mathrm{H}_{0}$ против альтернативы $\mathrm{H}_{1}$ называется \emph{критерием согласия}.
\end{definition}

Что мы хотели бы видеть от такого критерия? В идеале увидеть что-то наподобие равномерно наиболее мощного критерия. Но эта задача почти всегда полностью безнадёжна. Даже для хороших распределений, для которых выполнено свойство монотонного отношения правдоподобий, такого критерия может не быть. Например, для распределений $\mathrm{Bin}(1, \theta)$, $\mathrm{Exp}(\theta)$, $\mathcal{N}(\theta, 1)$ найти равномерно наиболее мощный критерий для проверки гипотезы $\mathrm{H}_{0} \colon \theta = \theta_{0}$ против альтернативы $\mathrm{H}_{1} \colon \theta \neq \theta_{0}$ не получится. С другой стороны, для некоторых распределений, для которых свойство монотонного отношения правдоподобий не выполнено, можно найти равномерно наиболее мощный критерий (например, $\mathrm{U}[0, \theta]$). Тем самым такая задача в некоторой степени осмысленна и для некоторых, казалось бы, сложных распределений её можно решить.

Для того, чтобы её решить, пока что приходится всё делать по-честному. Поэтому перейдём задаче поиска \emph{асимптотических критериев}. Такая постановка осмысленна: если не можем решить точно, то хотя бы решим приближенно. Пусть $\vec{X} = (X_{1}, \ldots, X_{n})$ "--- выборка растущего размера. Мы хотели бы получить последовательность критериев $S_{n} = S_{n}(X_{1}, \ldots, X_{n})$ для проверки гипотезы $\mathrm{H}_{0} \colon \theta = \theta_{0}$ против альтернативы $\mathrm{H}_{1} \colon \theta \neq \theta_{0}$ такую, что
\begin{itemize}
	\item Предел ошибки вероятности ошибки первого рода стремится к $\gamma \in (0, 1)$: $\Pr_{\theta_{0}}(\vec{X} \in S_{n}) \to \gamma$.
	\item Последовательность критериев будет состоятельна: для любого $\theta \neq \theta_{0}$ $\Pr_{\theta}(\vec{X} \in S_{n}) \to 1$.
\end{itemize}
Не смотря на то, что условия кажутся сложными для выполнения, оказывается, что решения есть. Рассмотрим один из таких критериев "--- критерий хи-квадрат Пирсона. Пусть есть выборка $\vec{X} = (X_{1}, \ldots, X_{n})$ из распределения $\Pr$ с конечным числом значений $\{a_{1}, \ldots, a_{m}\}$:
\begin{equation}
	p_{j} = \Pr(X_{1} = a_{j}) > 0, \quad j = 1, \ldots, m, \quad \sum_{i = 1}^{m} p_{i} = 1.
\end{equation}
Введём вектор вероятностей $\vec{p} = (p_{1}, \ldots, p_{m})$. Гипотеза $\mathrm{H}_{0} \colon F = F_{0}$ бужет сводиться к равенству векторов вероятностей, то есть задача согласия сводится к проверке гипотезы $\mathrm{H}_{0} \colon \vec{p} = \vec{p}^{0}$ против альтернативы $\mathrm{H}_{1} \colon \vec{p} \neq \vec{p}^{0}$. Для проверки данной гипотезы вводится \emph{статистика хи-квадрат}:
\begin{equation}
	\hat{\chi}^{2}_{n}(\vec{X}) = \sum_{j = 1}^{m} \frac{(\mu_{j} - np_{j}^{0})^{2}}{np_{j}^{0}}, \text{ где } \mu_{j} = \sum_{i = 1}^{n} \mathbf{1}_{X_{i} = a_{j}}.
\end{equation} 
Критерий же устроен следующим образом: пусть $u_{1 - \gamma}$ "--- $(1 - \gamma)$-квантиль распределения $\chi^{2}_{m - 1}$. Тогда гипотеза $\mathrm{H}_{0}$ отвергается тогда и только тогда, когда $\hat{\chi}^{2}_{n}(\vec{X}) > u_{1 - \gamma}$. Теперь докажем, что это хороший асимптотический критерий.
\begin{statement}
	Критерий хи-квадрат состоятелен.
\end{statement}
\begin{proof}
	Для начала немного перепишем статистику хи-квадрат:
	\begin{equation}
		\hat{\chi}^{2}_{n}(\vec{X}) = n\sum_{j = 1}^{m} \frac{1}{p_{j}^{0}}\left(\frac{\mu_{j}}{n} - p_{j}^{0}\right)^{2}.
	\end{equation}
	Далее, если $\vec{p} \neq \vec{p}^{0}$, то найдётся индекс $i \in \{1, 2, \ldots, m\}$ такой, что $p_{j} \neq p_{j}^{0}$. Однако по усиленному закону больших чисел $\mu_{j}/n \to p_{j}$ почти наверное. Отсюда следует, что $\hat{\chi}^{2}_{n}(\vec{X})$ линейно стремится к бесконечности, так как
	\begin{equation}
		\frac{\hat{\chi}^{2}_{n}(\vec{X})}{n} \xrightarrow{\text{п.н.}} \sum_{j = 1}^{m} \frac{(p_{j} - p_{j}^{0})^{2}}{p_{j}^{0}} > 0.
	\end{equation}
	Тем самым критерий состоятелен: $\Pr(\hat{\chi}^{2}_{n}(\vec{X}) > u_{1 - \gamma}) \to 1$.
\end{proof}
Первое свойство будет получаться из следующей теоремы:
\begin{theorem}[Пирсона]
	Если выполнена гипотеза $\mathrm{H}_{0}$, то имеет место следующая сходимость по распределению:
	\begin{equation}
		\hat{\chi}^{2}_{n}(\vec{X}) \xrightarrow[n \to \infty]{d} \chi^{2}_{m - 1}.
	\end{equation}
\end{theorem}
\begin{proof}
	Пусть $\vec{p} = \vec{p}^{0}$. Для всех $i = 1, \ldots, n$ введём случайный вектор $\vec{Y}_{i} = (\mathbf{1}_{X_{i} = a_{1}}, \ldots, \mathbf{1}_{X_{i} = a_{m}})$. Понятно, что такие векторы независимы и одинаково распределены, причём $\EE[\vec{Y}_{i}] = \vec{p}^{0}$. Далее заметим, что
	\begin{equation}
		\frac{\vec{Y}_{1} + \ldots + \vec{Y}_{n}}{n} = \frac{1}{n}\begin{pmatrix}
			\mu_{1} \\ \ldots \\ \mu_{m}
		\end{pmatrix}.
	\end{equation}
	Следовательно, по многомерной центральной предельной теореме
	\begin{equation}
		\sqrt{n}\left(\frac{\vec{Y}_{1} + \ldots + \vec{Y}_{n}}{n} - \vec{p}^{0}\right)
		\xrightarrow[n \to \infty]{d} \mathcal{N}(\vec{0}, \matr{\Sigma}),
	\end{equation} 
	где $\matr{\Sigma} = \DD[\vec{Y}_{i}]$. Заметим, что
	\begin{equation}
		\cov(\mathbf{1}_{X_{1} = a_{i}}, \mathbf{1}_{X_{1} = a_{j}})
		= \Pr(X_{1} = a_{i}, X_{1} = a_{j}) - p_{i}^{0}p_{j}^{0}
		= \begin{cases}
			p_{i}^{0} - p_{i}^{0}p_{j}^{0}, & i = j \\
			-p_{i}^{0}p_{j}^{0}, & i \neq j
		\end{cases}
	\end{equation}
	Тогда запишем матрицу ковариаций следующим образом: $\matr{\Sigma} = \matr{B} - \vec{p}^{0}(\vec{p}^{0})^{\top}$, где $\matr{B} = \matrixdiag(\vec{p}^{0})$. Теперь наша цель "--- поворочать вектор в ЦПТ так, чтобы матрица ковариаций стала разумной: нам нужно получить распределение хи-квадрат, поэтому желательно получить матрицу, чем-то похожую на матрицу стандартного нормального распределения). Пусть
	\begin{equation}
		\vec{\xi}_{n}^{'} = \sqrt{n}\left(\frac{\vec{Y}_{1} + \ldots + \vec{Y}_{n}}{n} - \vec{p}^{0}\right).
	\end{equation}
	Тогда по центральной предельной теореме
	\begin{equation}
		(\sqrt{\matr{B}})^{-1}\vec{\xi}_{n}^{'} \xrightarrow[n \to \infty]{d} (\sqrt{\matr{B}})^{-1}\mathcal{N}(\vec{0}, \matr{\Sigma}) = \mathcal{N}(\vec{0}, (\sqrt{\matr{B}})^{-1}\matr{\Sigma}(\sqrt{\matr{B}})^{-1}).
	\end{equation}
	Новая матрица ковариаций равна $\matr{I}_{m} - \vec{z}\vec{z}^{\top}$, где $\vec{z} = (\sqrt{\matr{B}})^{-1}\vec{p}^{0} = (\sqrt{p_{1}^{0}}, \ldots, \sqrt{p_{m}^{0}})$. Заметим, что вектор $\vec{z}$ единичный, а это означает, что его можно перевести поворотами в базисный вектор. Рассмотрим ортогональную матрицу $\matr{C}$, у которой первая строка равна $\vec{z}$, а остальные равны чему угодно. Тогда по теореме о наследовании сходимости
	\begin{equation}
		\vec{\xi}_{n}^{''} = \matr{C}(\sqrt{\matr{B}})^{-1}\xi_{n}^{'} \xrightarrow[n \to \infty]{d} \mathcal{N}(\vec{0}, \matr{C}(\matr{I}_{m} - \vec{z}\vec{z}^{\top})\matr{C}^{\top}).
	\end{equation}
	Однако $\matr{C}\vec{z} = (1, 0, \ldots, 0)^{\top}$, а $\matr{C}\matr{C}^{\top} = \matr{I}_{m}$. Поэтому $\matr{C}(\matr{I}_{m} - \vec{z}\vec{z}^{\top})\matr{C}^{\top} = \matrixdiag(0, 1, \ldots, 1) = \matr{I}^{'}_{m}$. Снова по теореме о наследовании сходимости
	\begin{equation}
		\|\vec{\xi}_{n}^{''}\|^{2} \xrightarrow[n \to \infty]{d} \|\mathcal{N}(\vec{0}, \matr{I}^{'}_{m})\|^{2} \sim \chi^{2}_{m - 1}.
	\end{equation}
	Но, как известно, помножение вектора на ортогональную матрицу не изменяет его норму. Тогда
	\begin{equation}
		\|(\sqrt{\matr{B}})^{-1}\vec{\xi}_{n}^{'}\|^{2} \xrightarrow[n \to \infty]{d} \chi^{2}_{m - 1}.
	\end{equation}
	Осталось заметить, что
	\begin{equation}
		(\sqrt{\matr{B}})^{-1}\vec{\xi}_{n}^{'}
		= (\sqrt{\matr{B}})^{-1}\sqrt{n}\begin{pmatrix}
			(\mu_{1} - np_{1}^{0})/n \\ \ldots  \\ (\mu_{m} - np_{m}^{0})/n
		\end{pmatrix}
		= \begin{pmatrix}
			(\mu_{1} - np_{1}^{0})/\sqrt{np_{1}^{0}} \\ \ldots  \\ (\mu_{m} - np_{m}^{0})/\sqrt{np_{m}^{0}}
		\end{pmatrix}
	\end{equation}
	Тем самым $\hat{\chi}^{2}_{n}(\vec{X}) = \|(\sqrt{\matr{B}})^{-1}\vec{\xi}_{n}^{'}\|^{2}$.
\end{proof}
Насколько этот метод применим? На практике считается, что должно быть следующее ограничение: $np_{j}^{0} \geq 5$ для всех $j = 1, \ldots, m$. 

\subsection{Параметрический хи-квадрат}
Метод достаточно мощный, хоть мы и пожертвовали тем, что вероятность ошибки первого рода не равна $\gamma$, а только стремится к ней. Что самое ироничное: отказ от точности дал некоторые хорошие свойства. Например, его можно применять, когда модель параметрическая. В таком случае критерий называется \emph{параметрическим критерием хи-квадрат}.

Пусть $\vec{X} = (X_{1}, \ldots, X_{n})$ "--- выборка из неизвестного распределения со значениями $\{a_{1}, \ldots, a_{m}\}$:
\begin{equation}
	p_{j}(\theta) = \Pr_{\theta}(X_{1} = a_{j}), \quad \theta \in \Theta
\end{equation}
В данном случае гипотеза и альтернатива формулируются следующим образом: $\mathrm{H}_{0} \colon \vec{p} \in \{\Pr_{\theta}, \theta \in \Theta\}$ против $\mathrm{H}_{1} \colon \vec{p} \not\in \{\Pr_{\theta}, \theta \in \Theta\}$. Теперь хотелось бы составить статистику хи-квадрат, но особо не получится, так как $\vec{p}^{0}$ больше нет. Можно ли его чем-то заменить? Заменим на оценку:
\begin{equation}
	\hat{\chi}^{2}_{n}(\vec{X}) = \sum_{i = 1}^{m} \frac{(\mu_{j} - n\hat{p}_{j}(\vec{X}))^{2}}{n\hat{p}_{j}(\vec{X})}.
\end{equation}
Какую оценку взять? Самым разумным выбором является оценка максимального правдоподобия: $\hat{p}_{j}(\vec{X}) = p_{j}(\hat{\theta}(\vec{X}))$, где $\hat{\theta}(\vec{X})$ "---  оценка максимального правдоподобия для $\theta$. Оказывается, что в таком случае можно доказать результат, похожий на теорему Пирсона: если $\vec{\theta} \in \mathbb{R}^{k}$, $k < m - 1$, то при выполнении определённых условий
\begin{equation}
	\hat{\chi}^{2}_{n}(\vec{X}) \xrightarrow[n \to \infty]{d} \chi^{2}_{m - 1 - k}.
\end{equation}
Данную теорему мы докажем на следующей лекции.