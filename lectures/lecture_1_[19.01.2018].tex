% !TeX root = ../main.tex

\section{Лекция 1}
\subsection{Ради чего мы собрались?}
Допустим, что у нас есть какое-то наблюдение $\vec{X}$, то есть какой-то случайный вектор (хотя в асимптотических случаях будем считать, что $\vec{X}$ бесконечномерен). 
Далее, распределение вектора $\vec{X}$ считается неизвестным.
Основная задача математической статистики состоит в том, чтобы \emph{выдать обоснованное мнение} о распределении $\vec{X}$.
Рассмотрим несколько основных подзадач:
\begin{enumerate}[label=(\alph*)]
	\item Допустим, что мы знаем, что искомое распределение $\Pr$ принадлежит какому-то классу распределений $\mathcal{P}$.
	Мы хотим проверить, можно ли выделить более узкий подкласс распределений $\mathcal{P}_{0} \subset \mathcal{P}$ такой, что $\Pr \in \mathcal{P}_{0}$.
	Данный вид задач называется \emph{проверкой гипотез}.
	
	\textbf{Пример:} допустим, что нам известно, что случайный вектор $X$ имеет нормальное распределение и мы хотим проверить гипотезу о том, что $X$ имеет нормальное распределение с нулевым матожиданием.
	
	\item Теперь предположим, что распределение $\Pr$ пришло из параметрического семейства: $\Pr \in \{\Pr_{\theta}\colon \theta \in \Theta\}$ и мы хотим оценить истинное значение параметра $\theta_{0}$, то есть построить оценку истинного значения $\hat{\theta}_{0}$.
	Такие задачи называются \emph{точечным оцениванием} (если мы оцениваем каким-то значением) или же \emph{интервальным оцениванием} (если мы в качестве оценки предлагаем какую-то область).
	
	\item Пусть наблюдение содержит пары: $\vec{X} = ((X_{1}, Y_{1}), \ldots, (X_{n}, Y_{n}))$.
	Возникает вопрос: можно ли сказать, что $Y_{i}$ и $X_{i}$ независимы? 
	
	\textbf{Пример:} Допустим, что мы собираем данные о цвете волос и цвете глаз у людей и пытаемся проверить, есть ли зависимость между ними.
	
	\item Допустим, что наблюдение разбилось на две части: $\vec{X} = (Y_{1}, \ldots, Y_{m}) \sqcup (Z_{1}, \ldots, Z_{n})$.
	Вопрос таков: можно ли сказать, что $Y_{i}$ и $Z_{j}$ равны по распределению?
	Другими словами, можно ли сказать, что в наблюдении действительно будут одинаково распределённые случайные величины? 
\end{enumerate}

\subsection{Точечное оценивание}
Начнём с точечного оценивания.
Пусть $\vec{X}$ "--- выборка (случайный вектор с независимыми и одинаково распределёнными компонентами) из неизвестного параметрически заданного распределения $\Pr \in \{\Pr_{\theta} \colon \theta \in \Theta\}$, причём параметром является набор из $k$ действительных чисел: $\Theta \subseteq \mathbb{R}^{k}$, $k \geq 1$.
По выборке мы должны каким-то образом оценить истинное значение параметра $\theta$.
Для этого строятся \emph{оценки}.
Дадим определение:
\begin{definition}
	\emph{Оценка} "--- это борелевская%
	\footnote{Напомню, что борелевская функция "--- это отображение такое, что для него полный прообраз борелевского множества будет борелевским множеством.} %
	функция $T \colon \mathcal{X} \mapsto \Theta$, где $\mathcal{X}$ "--- выборочное пространство.
	Проще говоря, это функция от выборки.
\end{definition}

Для оценок нет единого обозначения.
Если оценивается параметр $\theta$, то оценку обозначают через $\hat{\theta}(\vec{X})$, $\theta^{*}(\vec{X})$ или же $\tilde{\theta}(\vec{X})$ (и так далее, но суть ясна).

Мы научились делать какие-то предсказания насчёт значения $\theta$.
Но как проверить адекватность предсказания?
Для этого нужно выделить какие-то полезные свойства оценок и проверять их.
Выпишем четыре основных свойства.

\begin{definition}
	Пусть $\hat{\theta}(\vec{X})$ "--- это оценка параметра $\theta$.
	Тогда будем называть оценку $\hat{\theta}$ \emph{несмещённой}, если для любого $\theta \in \Theta$ 
	\[
		\EE_{\theta}[\hat{\theta}(\vec{X})] = \theta.
	\]
	В данном случае $\EE_{\theta}$ означает, что при взятии математического ожидания мы предполагаем, что выборка $\vec{X}$ взята из распределения $\Pr_{\theta}$.
\end{definition}

\begin{definition}
	Пусть размер выборки $\vec{X}$ увеличивается и $\hat{\theta}_{n}(\vec{X}) = \hat{\theta}_{n}(X_{1}, \ldots, X_{n})$.
	Тогда последовательность оценок $\hat{\theta}_{n}(\vec{X})$ будет называться \emph{состоятельной} оценкой параметра $\theta$, если для любого $\theta \in \Theta$
	\[
		\hat{\theta}_{n}(\vec{X}) \xrightarrow{\Pr_{\theta}} \theta.
	\]
\end{definition}

Понятие \emph{сильно состоятельной} оценки совпадает с определением состоятельной оценки с тем отличием, что в ней сходимость по вероятности заменяется на сходимость почти наверное:
\[
	\hat{\theta}_{n}(\vec{X}) \xrightarrow{\Pr_{\theta}\text{-п.н.}} \theta.
\]

\begin{definition}
	Пусть размер выборки $\vec{X}$ увеличивается и $\hat{\theta}_{n}(\vec{X}) = \hat{\theta}_{n}(X_{1}, \ldots, X_{n})$.
	Тогда последовательность оценок $\hat{\theta}_{n}(\vec{X})$ будет называться \emph{асимптотически нормальной} оценкой параметра $\theta$, если для любого $\theta \in \Theta$
	\[
		\sqrt{n}(\hat{\theta}_{n}(\vec{X}) - \theta) \xrightarrow{d_{\theta}} \mathcal{N}(\vec{0}, \matr{\Sigma}(\theta)),
	\]
	где $\matr{\Sigma}(\theta)$ "--- это \emph{асимптотическая дисперсия} $\hat{\theta}_{n}(\vec{X})$.
\end{definition}

Теперь приведём общую идею того, как можно строить оценки.
Допустим, что мы смогли найти функционал $G$ такой, что для всех $\theta \in \Theta$ $G(\Pr_{\theta}) = \theta$.
Тогда <<хорошей>> оценкой можно считать $\hat{\theta}(\vec{X}) = G(\Pr_{n}^{*})$, где $\Pr_{n}^{*}$ "--- это \emph{эмпирическое распределение}:%
\footnote{Здесь и в дальнейшем для обозначения индикатора будем использовать \emph{нотацию Айверсона}: $[P] = 1$, если $P$ правда и 0 иначе.}%
\[
	\Pr_{n}^{*}(B) = \frac{1}{n}\sum_{k = 1}^{n} [x_{i} \in B]
\]
На эмпирическое распределение можно смотреть, как на дискретное распределение, равновероятно сосредоточенное в точках выборки $\vec{X}$.
Теперь рассмотрим несколько примеров:
\begin{enumerate}[label=(\alph*)]
	\item Допустим, что функционал $G$ устроен по следующему правилу:
	\[
		G(\Pr_{\theta}) 
		= \int_{\mathcal{X}} g(x)\Pr_{\theta}(\diff x).
	\]
	Это интеграл Лебега по мере $\Pr_{\theta}$ от функции $g$.
	Людям, не знакомым с ним, можно читать его следующим образом: если $\Pr_{\theta}$ "--- это абсолютно непрерывная вероятностная мера с плотностью $p_{\theta}$, то
	\[
		\int_{\mathcal{X}} g(x)\Pr_{\theta}(\diff x) = \int_{\mathcal{X}} g(x)p_{\theta}(x)\diff x.
	\]
	Если же $\Pr_{\theta}$ есть дискретная вероятностная мера, то
	\[
		\int_{\mathcal{X}} g(x)\Pr_{\theta}(\diff x) = \sum_{x \in \mathcal{X}} g(x)\Pr_{\theta}(\{x\}).
	\]
	
	В таком случае оценка будет иметь вид
	\[
		G(\Pr_{n}^{*}) = \frac{1}{n}\sum_{i = 1}^{n} g(X_{i}) \equiv \overline{g(\vec{X})}.
	\]
	$\overline{g(\vec{X})}$ обычно называют \emph{выборочным средним}.
	
	\item Теперь скажем, что оценка имеет вид линейной комбинации функций от порядковых статистик (такие оценки обычно называют \emph{L-оценками}):
	\[
		\hat{\theta}_{n}(\vec{X}) = \sum_{i = 1}^{n} \alpha_{i} \varphi_{i}(X_{(i)}).
	\]
	На всякий случай напомним определение порядковой статистики.
	Пусть $\omega \in \Omega$ и $x_{k} = X_{k}(\omega)$. Далее, перенумеруем последовательность так, чтобы она шла по возрастанию: $x_{(1)} \leq x_{(2)} \leq \ldots \leq x_{(n)}$.
	Эту последовательность называют \emph{вариационным рядом}.
	Случайную величину $X_{(k)}(\omega) = x_{(k)}$ называют \emph{$k$-й порядковой статистикой}.
	Сразу же заметим, что $X_{(1)} = \min\{X_{1}, \ldots, X_{n}\}$ и $X_{(n)} = \max\{X_{1}, \ldots, X_{n}\}$.
	
	В качестве примера возьмём $G$ такой, что $G(\Pr_{\theta})$ будет равен $\alpha$-квантилю $\Pr_{\theta}$.
	Напомню, что $\alpha$-квантиль $c_{\alpha}$ функции распределения $F$ равен минимальному значению $x$, в котором $F(x) \geq \alpha$:
	\[
		c_{\alpha} = \min\{x \colon F(x) \geq \alpha\}.
	\]
	
	Тогда \emph{выборочным $\alpha$-квантилем} назовём 
	\[
		G(\Pr_{n}^{*}) = \begin{cases}
		X_{(n\alpha)}, & n\alpha \text{ целое} \\
		X_{(\lceil n\alpha \rceil)}, & n\alpha \text{ не целое}
		\end{cases}
	\]
	
	\item Теперь скажем, что функционал $G$ устроен следующим образом:
	\[
		G(\QQ) = \arg\min_{\theta \in \Theta} \int_{\mathcal{X}} \psi(x, \theta)\QQ(\diff x).
	\]
	
	Такие оценки называют \emph{M-оценками}. Тогда
	\[
		G(\Pr_{n}^{*}) = \arg\min_{\theta \in \Theta} \frac{1}{n}\sum_{k = 1}^{n} \psi(X_{k}, \theta).
	\]
	
	В качетсве примера M-оценки можно взять оценку максимального правдоподобия: в ней $\psi(x, \theta) = -\log p_{\theta}(x)$.
\end{enumerate}

Попробуем разобраться, в каких задачах можно построить <<хорошие>> оценки в том смысле, что они будут обладать свойствами, введённых выше. 

Кто может определять распределение?
Первая мысль, которая приходит в голову "--- моменты.
Из этого мы сразу получаем так называемый \emph{метод моментов}.
Но этот метод плох: у него есть много недостатков, а плюс один "--- на него можно давать задачи на контрольной.
Основная сложность состоит в том, что нужно уметь считать моменты, как функции от параметра, что может быть совсем нетривиально или же невозможно (особенно в случае, когда распределение далеко не табличное).

Для примера рассмотрим распределение Коши $\mathrm{C}(\theta, 1)$.
Обычно его задают плотностью $f(x; \theta, 1)$, которая равна
\[
	f(x; \theta, 1) = \frac{1}{\pi(1 + (x - \theta)^{2})}.
\]
Если взять случайную величину с таким распределением, то у неё не будет матожидания, дисперсии да и всех моментов выше первого.
Конечно, можно посчитать матожидание логарифма (в таком случае интеграл сойдётся), но выразить её, как явную функцию от $\theta$, не получится.
Небольшое примечание: в данном распределении $\theta$ выступает в качестве медианы.

Второй подход предлагает использовать квантили.
Очевидно, что этот подход универсален, так как по любой функции распределения можно построить квантили.
Но у этого метода есть сложность "--- квантили считать ещё сложнее, чем моменты.
Так что этот метод в основном подходит только для определения <<сдвига>>: если плотность симметрична относительно какой-то точки, то эта точка служит медианой, а медиану на практике считать не так уж и сложно.

Ещё одна идея завязывается на M-оценках.
Описать её явно не так уж и просто, но общую философию можно сказать так: <<мы живём в наиболее вероятном мире>>.
Допустим, мы посчитали функционал и определили $\theta$ при фиксированном $\vec{X}$, при котором он максимален (или минимален).
Тогда эту $\theta$ берут в качестве настоящей, так как мы считаем, что она отражает действительность.

\subsection{Метод моментов}
Пусть $\vec{X}$ "--- выборка из распределения $\Pr \in \{\Pr_{\theta} \colon \theta \in \Theta\}$, причём $\Theta \subseteq \mathbb{R}^{k}$.
Далее, возьмём \emph{пробные функции} $g_{1}(x), \ldots, g_{k}(x)$ такие, что вектор
\[
	m(\theta) = (\EE_{\theta}[g_{1}(X_{1})], \ldots, \EE_{\theta}[g_{k}(X_{1})])
\]
задаёт биекцию между $\Theta$ и $m(\Theta)$.
Тогда оценкой $\hat{\theta}(\vec{X})$ по методу моментов с пробными функциями $g_{1}(x), \ldots, g_{k}(x)$ называется решение системы уравнений (относительно $\theta$)
\[
	\begin{cases}
		m_{1}(\theta) = \overline{g_{1}(\vec{X})} \\
		\ldots \\
		m_{k}(\theta) = \overline{g_{k}(\vec{X})}
	\end{cases}
\]
Здесь $m_{j}(\theta) = \EE_{\theta}[g_{j}(X_{1})]$. Так как $m(\theta)$ задаёт биекцию, то
\[
	\hat{\theta}(\vec{X}) = m^{-1}(\overline{g_{1}(\vec{X})}, \ldots, \overline{g_{k}(\vec{X})}).
\]
Обычно в качестве пробных функций берут $x, x^{2}, \ldots, x^{k}$.

Оказывается, что с теоретической точки зрения оценки с помощью метода моментов не так уж и плохи.
\begin{lemma}
	Пусть $\hat{\theta}_{n}(\vec{X})$ "--- это оценка $\theta$ по методу моментов. Тогда
	\begin{enumerate}[label=(\alph*)]
		\item Если $m^{-1}$ непрерывна, то $\hat{\theta}_{n}(\vec{X})$ есть сильно состоятельная оценка для $\theta$.
		\item Если $m^{-1}$ дифференцируема и $\EE_{\theta}[g_{j}^{2}(X_{1})] < +\infty$ для всех $j = 1, \ldots, k$, то $\hat{\theta}_{n}(\vec{X})$ есть асимптотически нормальная оценка $\theta$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	Для начала заметим, что по усиленному закону больших чисел
	\[
		\overline{g_{j}(\vec{X})} = \frac{1}{n}\sum_{k = 1}^{n} g_{j}(X_{k}) \xrightarrow{\Pr_{\theta}\text{-п.н.}} \EE_{\theta}[g_{j}(X_{1})].
	\]
	Следовательно, по теореме о наследовании сходимости
	\[
		\hat{\theta}_{n}(\vec{X}) = m^{-1}(\overline{g_{1}(\vec{X})}, \ldots, \overline{g_{k}(\vec{X})}) \xrightarrow{\Pr_{\theta}\text{-п.н.}} m^{-1}(m_{1}(\theta), \ldots, m_{k}(\theta)) = m^{-1}(m(\theta)) = \theta.
	\]
	
	Для доказательства асимптотической нормальности заметим следующее.
	Введём следующие векторы: $\vec{Y}_{i} = (g_{1}(X_{i}), \ldots, g_{k}(X_{i}))$.
	Далее, так как элементы выборки независимы и одинаково распределены, то $\vec{Y}_{i}$ тоже независимы и одинаково распределены.
	Так как мы предполагаем, что $\EE_{\theta}[g_{j}^{2}(X_{1})] < + \infty$, то матрица ковариаций $\matr{\Sigma}$ невырождена.
	Тогда мы можем применить многомерную центральную предельную теорему:
	\[
		\sqrt{n}\left(\frac{\vec{Y}_{1} + \ldots + \vec{Y}_{n}}{n} - \EE_{\theta}[\vec{Y}_{1}]\right) \xrightarrow{d_{\theta}} \mathcal{N}(\vec{0}, \matr{\Sigma}(\theta))
	\]
	Для дальнейших рассуждений нужна следующая
	\begin{lemma}[о наследовании асимптотической сходимости]
		Пусть $\xi_{i} \in \mathbb{R}^{m}$ "--- последовательность случайных векторов, для которых существует вектор $\vec{a} \in \mathbb{R}^{n}$ и матрица $\matr{\Sigma} \in \mathbb{R}^{n \times n}$ такие, что 
		\[
			\sqrt{n}(\xi_{n} - \vec{a}) \xrightarrow{d_{\theta}} \mathcal{N}(\vec{0}, \matr{\Sigma}).
		\]
		Далее, пусть $H \colon \mathbb{R}^{m} \mapsto \mathbb{R}^{n}$ "--- это дифференцируемая функция. Тогда
		\[
			\sqrt{n}(H(\xi_{n}) - H(\vec{a})) \xrightarrow{d_{\theta}} H'(\vec{a})\mathcal{N}(\vec{0}, \matr{\Sigma}),
		\]
		где $H'(\vec{a})$ "--- это матрица Якоби функции $H$ в точке $\vec{a}$.
	\end{lemma}
	Теперь применим эту лемму, положив $H = m^{-1}$:
	\[
		\sqrt{n}\left(m^{-1}\left(\frac{\vec{Y}_{1} + \ldots + \vec{Y}_{n}}{n}\right) - m^{-1}(\EE_{\theta}[\vec{Y}_{1}])\right) = \sqrt{n}(\hat{\theta}_{n}(\vec{X}) - \theta) \xrightarrow{d_{\theta}} (m^{-1})'(m(\theta))\mathcal{N}(\vec{0}, \matr{\Sigma}(\theta)).
	\]
	Осталось заметить, что $(m^{-1})'(m(\theta))\mathcal{N}(\vec{0}, \matr{\Sigma}(\theta)) = \mathcal{N}(\vec{0}, \matr{A}(\theta))$ для некоторой матрицы $\matr{A}(\theta)$.
\end{proof}

\subsection{Выборочные квантили}
Теперь приступим ко второму методу описания функции распределения "--- через квантили.
Оказывается, что у выборочных квантилей есть очень хорошее свойство.
\begin{theorem}
	Пусть $\vec{X} = (X_{1}, \ldots, X_{n})$ "--- выборка из распределения $F(x)$ с плотностью $f(x)$, а $z_{p}$ "--- $p$-квантиль распределения $F(x)$.
	Далее, пусть $f(x)$ непрерывно дифференцируема в окрестности $z_{p}$ и $f(z_{p}) > 0$.
	Тогда выборочный квантиль есть асимптотически нормальная оценка теоретического квантиля:
	\[
		\sqrt{n}(X_{[np] + 1} - z_{p}) \xrightarrow{d_{\theta}} \mathcal{N}\left(0, \frac{p(1 - p)}{f^{2}(z_{p})}\right).
	\] 
\end{theorem}
\begin{proof}
	Начнём с того, что посчитаем плотность $k$-й порядковой статистики.
	Для этого заметим, что если $X_{(k)} \leq x$, то хотя бы $k$ элементов выборки не больше $x$.
	Тогда, если $F_{k}(x)$ "--- это функция распределения $k$-й статистики, то
	\[
		F_{k}(x) = \Pr(X_{(k)} \leq x) = \sum_{s = k}^{n} \binom{n}{s}F^{s}(x)(1 - F(x))^{n - s}.
	\]
	Теперь посчитаем плотность $f_{k}(x)$ функции распределения $F_{k}(x)$, продифференцировав её:
	\[
		f_{k}(x) = \sum_{s = k}^{n} sf(x)\binom{n}{s}F^{s - 1}(x)(1 - F(x))^{n - s} - \sum_{s = k}^{n - 1} (n - s)f(x)\binom{n}{s}F^{s}(x)(1 - F(x))^{n - s - 1}.
	\]
	Осталось упростить эту сумму.
	Для этого заметим, что
	\[
		s\binom{n}{s} = \frac{n!}{(s - 1)!(n - s)!} = n\binom{n - 1}{s - 1}, \quad (n - s)\binom{n}{s} = \frac{n!}{s!(n - s - 1)!} = n\binom{n - 1}{s}.
	\]
	Тогда, заменив индекс суммирования в первой сумме на $t = s - 1$, получим следующее:
	\[
		f_{k}(x) = \sum_{t = k - 1}^{n - 1} nf(x)\binom{n - 1}{t}F^{t}(x)(1 - F(x))^{n - t - 1} - \sum_{t = k}^{n - 1} nf(x)\binom{n - 1}{t}F^{t}(x)(1 - F(x))^{n - t - 1}.
	\]
	Но тогда
	\[
		f_{k}(x) = nf(x)\binom{n - 1}{k - 1}F^{k - 1}(x)(1 - F(x))^{n - k}.
	\]
	
	Теперь покажем, что если мы возьмём последовательность случайных величин $T_{n}$, построенных по правилу
	\[
		T_{n} = \frac{f(z_{p})\sqrt{n}}{\sqrt{p(1 - p)}}(X_{(k)} - z_{p}), \text{ где } k = [np] + 1,
	\]
	то $T_{n} \xrightarrow{d_{\theta}} \mathcal{N}(0, 1)$.
	Для этого покажем один промежуточный факт: если $\xi$ "--- случайная величина из распределения с плотностью $f_{\xi}$, а $\eta = a\xi + b$, где $a > 0$ и $b$ "--- константы, то $\eta$ имеет плотность $f_{\eta}$, равную
	\[
		f_{\eta}(x) = \frac{1}{a}f_{\xi}\left(\frac{x - b}{a}\right).
	\]
	Это несложно понять, если заметить следующее:
	\[
		F_{\eta}(x) = \Pr(\eta \leq x) = \Pr(a\xi + b \leq x) = \Pr\left(\xi \leq \frac{x - b}{a}\right) = F_{\xi}\left(\frac{x - b}{a}\right).
	\]
	Дифференцируя по $x$, мы получим желаемое.
	Воспользуемся этим.
	Пусть $q_{n}(x)$ "--- плотность $T_{n}$.
	Тогда
	\[
		q_{n}(x) = \frac{\sqrt{p(1 - p)}}{f(z_{p})\sqrt{n}}f_{k}\left(z_{p} + \frac{x\sqrt{p(1 - p)}}{f(z_{p})\sqrt{n}}\right).
	\]
	Для удобства введём следующее обозначение:
	\[
		t_{n} = z_{p} + \frac{x\sqrt{p(1 - p)}}{f(z_{p})\sqrt{n}}.
	\]
	Тогда
	\[
		q_{n}(x) = n\binom{n - 1}{k - 1}F(t_{n})^{k - 1}f(t_{n})(1 - F(t_{n}))^{n - k}\sqrt{\frac{p(1 - p)}{n}}\frac{1}{f(z_{p})}.
	\]
	Теперь сделаем финт ушами и скажем, что
	\[
		q_{n}(x) = A_{1}(n)A_{2}(n)A_{3}(n),
	\]
	где
	\begin{align*}
		A_{1}(n) &= \frac{f(t_{n})}{f(z_{p})}, \\
		A_{2}(n) &= n\binom{n - 1}{k - 1}\sqrt{\frac{p(1 - p)}{n}}p^{k - 1}(1 - p)^{n - k}, \\
		A_{3}(n) &= \left(\frac{F(t_{n})}{p}\right)^{k - 1}\left(\frac{1 - F(t_{n})}{1 - p}\right)^{n - k}.
	\end{align*}
	
	Найдём пределы всех трёх выражений при $n \to \infty$:
	\begin{enumerate}
		\item В первом всё очень просто: достаточно заметить, что $t_{n} \to z_{p}$ при $n \to \infty$ и $f(x)$ непрерывна в окрестности $z_{p}$.
		Тогда
		\[
			\frac{f(t_{n})}{f(z_{p})} \xrightarrow[n \to \infty]{} 1 \implies \lim_{n \to \infty} A_{1}(n) = 1.
		\]
		
		\item Для этого пункта заметим, что
		\[
			A_{2}(n) = k\binom{n}{k}\sqrt{\frac{p(1 - p)}{n}}p^{k - 1}(1 - p)^{n - k}.
		\]
		Далее, воспользуемся формулой Стирлинга:
		\[
			A_{2}(n) \sim k\frac{\sqrt{2\pi n}(\frac{n}{e})^{n}}{\sqrt{2\pi k}(\frac{k}{e})^{k}\sqrt{2\pi(n - k)}(\frac{n - k}{e})^{k}}\sqrt{\frac{p(1 - p)}{n}}p^{k - 1}(1 - p)^{n - k}.
		\]
		Теперь вспомним, что $k = [np] + 1$.
		Но из этого следует, что $k \sim np$ и $n - k \sim n(1 - p)$.
		Тогда
		\[
			A_{2}(n) \sim np\frac{\sqrt{2\pi n}(\frac{n}{e})^{n}}{\sqrt{2\pi np}(\frac{k}{e})^{k}\sqrt{2\pi n(1 - p)}(\frac{n - k}{e})^{k}}\sqrt{\frac{p(1 - p)}{n}}p^{k - 1}(1 - p)^{n - k}.
		\]
		Упростим:
		\[
			A_{2}(n) \sim \frac{1}{\sqrt{2\pi}}\left(\frac{np}{k}\right)^{k}\left(\frac{n(1 - p)}{n - k}\right)^{n - k}.
		\]
		Теперь докажем, что $A_{2}(n) \to 1/\sqrt{2\pi}$ при $n \to \infty$.
		Заметим, что
		\[
			A_{2}(n) \sim \frac{1}{\sqrt{2\pi}}\exp\left\{k\ln\frac{np}{k} + (n - k)\ln\frac{n(1 - p)}{n - k}\right\}.
		\]
		Далее, $k$ отличается от $np$ не более, чем на 1.
		Тогда логарифмы можно разложить в ряд Тейлора в нуле:
		\begin{align*}
			\ln \frac{np}{k} 
			&= \frac{np}{k} - 1 + O\left(\frac{1}{k^{2}}\right) 
			= \frac{np - k}{k} + O\left(\frac{1}{n^{2}}\right), \\
			\ln \frac{n(1 - p)}{n - k} &= \frac{n(1 - p)}{n - k} - 1 + O\left(\frac{1}{(n - k)^{2}}\right) = \frac{k - np}{n - k} + O\left(\frac{1}{n^{2}}\right).
		\end{align*}
		
		Следовательно,
		\[
			A_{2}(n) \sim \frac{1}{\sqrt{2\pi}}\exp\left\{np - k + k - np + O\left(\frac{1}{n^{2}}\right)\right\} \xrightarrow[n \to \infty]{} \frac{1}{\sqrt{2\pi}}.
		\]
		
		\item Для этого пункта нужно заметить, что $F(t_{n}) \to F(z_{p}) = p$ при $n \to \infty$.
		Из этого можно сделать вывод, что
		\[
			\left(\frac{F(t_{n})}{p}\right)^{k - 1} \sim \left(\frac{F(t_{n})}{p}\right)^{k}.
		\]
		Далее, заметим, что
		\[
			A_{3}(n) \sim \exp\left\{k\ln\frac{F(t_{n})}{p} + (n - k)\ln\frac{1 - F(t_{n})}{1 - p}\right\}.
		\]
		Теперь разложим $F(t_{n})$ в ряд Тейлора в точке $z_{p}$, пользуясь непрерывной дифференцируемостью $f(x)$:
		\begin{align*}
			F(t_{n}) 
			&= F(z_{p}) + (t_{n} - z_{p})f(z_{p}) + \frac{1}{2}(t_{n} - z_{p})^{2}f'(z_{p}) + o((t_{n} - z_{p})^{2}) \\
			&= p + \sqrt{\frac{p(1 - p)}{n}} x + \frac{f'(z_{p})}{f^{2}(z_{p})}\frac{p(1 - p)}{2n}x^{2} + o((t_{n} - z_{p})^{2}).
		\end{align*}
		Тогда
		\[
			\frac{F(t_{n})}{p} = 1 + \sqrt{\frac{1 - p}{np}}x + \frac{f'(z_{p})}{f^{2}(z_{p})}\frac{1 - p}{2n}x^{2} + o\left(\frac{1}{n}\right)
		\]
		
		Следовательно, если взять от этого логарифм, то его можно разложить в ряд Тейлора:
		\[
			\ln \frac{F(t_{n})}{p} = \sqrt{\frac{1 - p}{np}}x + \frac{f'(z_{p})}{f^{2}(z_{p})}\frac{1 - p}{2n}x^{2} + o\left(\frac{1}{n}\right) - \frac{1}{2}\frac{1 - p}{np}x^{2} + o\left(\frac{1}{n}\right).
		\]
		Тогда
		\[
			k\ln \frac{F(t_{n})}{p} = \sqrt{\frac{1 - p}{np}}xk + \frac{f'(z_{p})}{f^{2}(z_{p})}\frac{k(1 - p)}{2n}x^{2} - \frac{1}{2}\frac{1 - p}{p}\frac{k}{n}x^{2} + o\left(1\right).
		\]
		
		Аналогичными расуждениями можно показать, что
		\[
			(n - k)\ln \frac{1 - F(t_{n})}{1 - p} = -\sqrt{\frac{p}{n(1 - p)}}x(n - k) - \frac{f'(z_{p})}{f^{2}(z_{p})}\frac{p(n - k)}{2n}x^{2} - \frac{1}{2}\frac{p}{1 - p}\frac{n - k}{n}x^{2} + o(1).
		\]
		Теперь воспользуемся тем, что $|k - np| \leq 1$.
		Это означает, что $k = np + O(1/n)$.
		Тогда
		\begin{align*}
			k\ln \frac{F(t_{n})}{p} 
			&= \sqrt{\frac{1 - p}{np}}x\left(np + O\left(\frac{1}{n}\right)\right) + \frac{1}{2}\frac{f'(z_{p})}{f^2(z_{p})}\frac{1 - p}{n}x^{2}\left(np + O\left(\frac{1}{n}\right)\right) \\
			&\phantom{=}-\frac{1}{2}\frac{1 - p}{p}\frac{x^{2}}{n}\left(np + O\left(\frac{1}{n}\right)\right) + o(1) \\
			&= x\sqrt{np(1 - p)} + O\left(\frac{1}{\sqrt{n}}\right) + \frac{p(1 - p)}{2}\frac{f'(z_{p})}{f^{2}(z_{p})}x^{2} + O\left(\frac{1}{n^{2}}\right) \\
			&\phantom{=}-\frac{1 - p}{2}x^{2} + O\left(\frac{1}{n^{2}}\right) + o(1) \\
			&= x\sqrt{np(1 - p)} + \frac{p(1 - p)}{2}\frac{f'(z_{p})}{f^{2}(z_{p})}x^{2} - \frac{1 - p}{2}x^{2} + o(1).
		\end{align*}
		Аналогично поступая со вторым слагаемым, получим, что
		\[
			(n - k)\ln \frac{1 - F(t_{n})}{1 - p} = -x\sqrt{np(1 - p)} - \frac{p(1 - p)}{2}\frac{f'(z_{p})}{f^{2}(z_{p})}x^{2} - \frac{p}{2}x^{2} + o(1).
		\]
		Следовательно,
		\[
			A_{3}(n) \sim \exp\left\{-\frac{x^{2}}{2} + o(1)\right\} \xrightarrow[n \to \infty]{} \exp\left\{-\frac{x^{2}}{2}\right\}.
		\]
	\end{enumerate}
	
	В итоге мы получили, что для всех $x$
	\[
		\lim\limits_{n \to \infty} q_{n}(x) = \frac{1}{\sqrt{2\pi}}\exp\left\{-\frac{x^{2}}{2}\right\}.
	\]
	Это означает, что $q_{n}(x)$ будет равномерно сходится к плотности $\mathcal{N}(0, 1)$ на любом компакте.
	Из равномерной сходимости на отрезке $[a, b]$ следует, что
	\[
		\lim\limits_{n \to \infty} (F_{T_{n}}(b) - F_{T_{n}}(a)) = \lim\limits_{n \to \infty} \int_{a}^{b} q_{n}(x)\diff x = \Phi(b) - \Phi(a).
	\]
	
	Теперь нужно доказать, что $F_{T_{n}}(x) \to \Phi(x)$ для всех $x$, где $\Phi(x)$ "--- это стандартное нормальное распределение.
	Заметим, что для любого $a < b$
	\[
		\varlimsup\limits_{n \to \infty} |F_{T_{n}}(b) - \Phi(b)| \leq \varlimsup\limits_{n \to \infty} |F_{T_{n}}(b) - F_{T_{n}}(a) + \Phi(b) - \Phi(a)| + \varlimsup\limits_{n \to \infty} |F_{T_{n}}(a) - \Phi(a)|
	\]
	Далее, первый предел уходит в ноль.
	Тогда
	\[
		\varlimsup\limits_{n \to \infty} |F_{T_{n}}(b) - \Phi(b)| \leq \varlimsup\limits_{n \to \infty} |F_{T_{n}}(a) - \Phi(a)|.
	\]
	Однако
	\[
		\varlimsup\limits_{n \to \infty} |F_{T_{n}}(a) - \Phi(a)| \leq \varlimsup\limits_{n \to \infty} (F_{T_{n}}(a) + \Phi(a)) = \Phi(a) + \varlimsup\limits_{n \to \infty} F_{T_{n}}(a).
	\]
	Далее, снова сделаем финт ушами, пользуясь тем, что $F_{T_{n}}(a) < 1$:
	\[
		\Phi(a) + \varlimsup\limits_{n \to \infty} F_{T_{n}}(a) \leq \Phi(a) + \varlimsup\limits_{n \to \infty} (F_{T_{n}}(a) - F_{T_{n}}(-a) + 1) = \Phi(a) + \Phi(a) - \Phi(-a) + 1.
	\]
	Но эту сумму можно сделать сколь угодно малой, устремив $a$ к $-\infty$.
	Следовательно, $T_{n} \xrightarrow{d_{\theta}} \mathcal{N}(0, 1)$ и мы получаем желаемое.
\end{proof}

Посмотрим на частный случай квантили, а именно на медиану.
Стоит заметить, что выборочную медиану определяют не как выборочный $1/2$-квантиль:
\begin{definition}
	Выборочной медианой $\hat{\mu}$ выборки $X_{1}, \ldots, X_{n}$ называют величину
	\[
		\hat{\mu}(\vec{X}) = \begin{cases}
		X_{(k + 1)}, & n = 2k + 1 \\
		(X_{(k)} + X_{(k + 1)})/2, & n = 2k
		\end{cases}
	\]
\end{definition}
Оказывается, что для такого определения свойство асимптотической нормальности тоже выполняется:
\begin{exercise}
	В условиях теоремы про асимптотическую нормальность выборочной квантили
	\[
		\sqrt{n}(\hat{\mu}(\vec{X}) - z_{1/2}) \xrightarrow{d_{\theta}} \mathcal{N}\left(0, \frac{1}{4f^{2}(z_{1/2})}\right).
	\]
\end{exercise}
В качестве примера рассмотрим выборку $\vec{X}$ из распределения Коши $\mathrm{C}(\theta, 1)$.
У данного распределения всё очень плохо с моментами, но при этом асимптотическая нормальность выборочной медианы выполняется:
\[
	\sqrt{n}(\hat{\mu}(\vec{X}) - \theta) \xrightarrow{d_{\theta}} \mathcal{N}\left(0, \frac{\pi^{2}}{4}\right).
\]